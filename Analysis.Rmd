---
title: "Shelter Animal Outcomes"
author: "Gabriel Lapointe"
date: "May 13, 2016"
output: html_document
variant: markdown_github
---

# 1 Introduction
Every year, approximately 7.6 million companion animals end up in US shelters. Many animals are given up as unwanted by their owners, while others are picked up after getting lost or taken out of cruelty situations. Many of these animals find forever families to take them home, but just as many are not so lucky. 2.7 million dogs and cats are euthanized in the US every year.


## 1.1 Objective
The objective is to help improving outcomes for shelter animals. Using a dataset of intake information including breed, color, sex, and age from the Austin Animal Center, we have to predict the outcome for each animal.

To support our analysis, we are given the train and test sets in CSV files. From this, we have written a code book to help understanding each feature and their values.


## 1.2 Data Source
The data comes from [Austin Animal Center](http://www.austintexas.gov/department/animal-services) from October 1st, 2013 to March, 2016. Outcomes represent the status of animals as they leave the Animal Center. All animals receive a unique Animal ID during intake. Source of the competition: [Shelter Animal Outcomes](https://www.kaggle.com/c/shelter-animal-outcomes).


## 1.3 Dataset Questions
Before we start the exploration of the dataset, we need to write a list of questions about this dataset considering the problem we have to solve. 

* How big is the dataset?
* Does the dataset contains `NA` or missing values? Can we replace them by a value?
* Does the data is coherent (date with same format, no out of bound values, no misspelled words, etc.)?
* What does the data look like and what are the relationships between features if they exist?
* What are the measures used?
* Can we solve the problem with this dataset?

Questions on features:

* What is the proportion of animals for each outcome?
* Are there features that can be split in many other features? If yes, are they improving the score? Why and how?
* For each outcome, what features have the most importance and why?


## 1.4 Evaluation Metrics
Since we have 5 outcome types where we need to calculate a prediction's probability for each class (outcome type), we have to manage 5 classes using a multi-class algorithm. The `Log Loss` function quantifies the accuracy of a classifier by penalising false classifications. Minimising the `Log Loss` function is basically equivalent to maximising the accuracy of the classifier which is what we need.

Thus, Kaggle provides us the evaluation metric we need:
$$LogLoss = -\frac{1}{N} \sum_{i = 0}^N \sum_{j = 0}^M y_{ij} \log{\mathbb{P}_{ij}}$$
where 
    $N$ is the total number of animals, 
    $M$ is the number of outcomes, 
    $y_{ij}$ is 1 if observation $i$ is in outcome $j$ and 0 otherwise,
    $\mathbb{P}_{ij}$ is the predicted probability that observation $i$ belongs to outcome $j$.


# 2 Exploratory Analysis
In this section, we explore the dataset and we test hypotheses on features. The objective is to visualize and understand the dataset we have to solve the problem. We will also compare changes we will make to this dataset to validate if they have significant influance on the outcomes or not.

We load the test and train datasets, and set the seed. All blank, space and unknown values are replaced by the value `0`. This will be explained in the next section about feature engineering.

```{r echo = TRUE, message = FALSE, warning = FALSE}
source("Utilities.R")

train <- read.csv("train.csv", header = TRUE, na.strings = c("", " ", "Unknown"), stringsAsFactors = FALSE)
test <- read.csv("test.csv", header = TRUE, na.strings = c("", " ", "Unknown"), stringsAsFactors = FALSE)

set.seed(1234)

train[is.na(train)] <- 0
test[is.na(test)] <- 0
```

Regarding the features possible values, we will use bar charts to visualize our analysis of the dataset.


### 2.1 Convert AgeuponOutcome Values to Days
We transform the feature `AgeuponOutcome` to integer values. The age will be counted in days. For example, `2 year` will be replaced by the value 2 * 365 = 730. For months, the formula will be `age * 30`. For weeks, the formula will be `age * 7`. Finally, for years, the formula will be `age * 365`.

```{r echo = TRUE, message = FALSE, warning = FALSE}
## Get the list of integer extracted from the feature AgeuponOutcome.
train.age <- as.integer(regmatches(train$AgeuponOutcome, regexpr("[[:digit:]]+", train$AgeuponOutcome)))
test.age <- as.integer(regmatches(test$AgeuponOutcome, regexpr("[[:digit:]]+", test$AgeuponOutcome)))

## Get row index list where AgeuponOutcome contains "year", "month", "week" or "day".
## Get the correspondant integer from each row index and apply the formula to get all ages in years.
train.year.list <- grep("year", train$AgeuponOutcome)
train$AgeuponOutcome[train.year.list] <- train.age[train.year.list] * 365
train.month.list <- grep("month", train$AgeuponOutcome)
train$AgeuponOutcome[train.month.list] <- train.age[train.month.list] * 30
train.week.list <- grep("week", train$AgeuponOutcome)
train$AgeuponOutcome[train.week.list] <- train.age[train.week.list] * 7
train.day.list <- grep("day", train$AgeuponOutcome)
train$AgeuponOutcome[train.day.list] <- train.age[train.day.list]

test.year.list <- grep("year", test$AgeuponOutcome)
test$AgeuponOutcome[test.year.list] <- test.age[test.year.list] * 365
test.month.list <- grep("month", test$AgeuponOutcome)
test$AgeuponOutcome[test.month.list] <- test.age[test.month.list] * 30
test.week.list <- grep("week", test$AgeuponOutcome)
test$AgeuponOutcome[test.week.list] <- test.age[test.week.list] * 7
test.day.list <- grep("day", test$AgeuponOutcome)
test$AgeuponOutcome[test.day.list] <- test.age[test.day.list]

train$AgeuponOutcome <- as.integer(train$AgeuponOutcome)
test$AgeuponOutcome <- as.integer(test$AgeuponOutcome)

age.summary <- summary(train$AgeuponOutcome)
print(age.summary)
```

Lets see the distribution of the age with an histogram.

```{r echo = TRUE, message = FALSE, warning = FALSE}
train.copy <- train
train.copy$AgeuponOutcome[train.copy$AgeuponOutcome < 365] <- 0
train.copy$AgeuponOutcome <- train.copy$AgeuponOutcome / 365
ggplot(train.copy, aes(x = AgeuponOutcome)) + 
    geom_histogram(aes(y = ..density..), binwidth = 1, colour = "black", fill = "white") +
    labs(title = "Density in function of the Age", y = "Density", x = "Age in days")
```

We can see that the age can be approximated by the exponential distribution from the density shown in the histogram above. 


## 2.2 Outcomes Visualization
The objective is to answer the question: `What is the proportion of animals for each outcome?`. Let's visualize how the outcomes are split in the train set with the following histogram.

```{r echo = TRUE, message = FALSE, warning = FALSE}
ggplot(train, aes(OutcomeType)) +
        geom_bar(aes(y = ..count.. / sum(..count..))) +
        scale_y_continuous(labels = percent) +
        geom_text(aes(y = ..count.. / sum(..count..), label = scales::percent((..count..) / sum(..count..))), stat = "count", vjust = -0.25) +
        labs(title = "Percentage for each outcome from the train set", y = "Percentage", x = "Outcome Types")
```


### 2.3 Named vs Not Named Animals
The feature `Name` is transformed to a boolean value where the value is 0 when the animal has no name, and 1 otherwise. Logically, the name of an animal should not have any impact on the outcomes. But knowing that an animal has no name versus has a name may influance the outcomes. 

```{r echo = TRUE, message = FALSE, warning = FALSE}
train$Name[train$Name != 0] <- 1
test$Name[test$Name != 0] <- 1

train$NameLength <- ifelse(train$Name == 0, 0, str_length(train$Name))
test$NameLength <- ifelse(test$Name == 0, 0, str_length(test$Name))
```

Let's take a look on the plot if this has a signifiant influance on the outcomes.

```{r echo = TRUE, message = FALSE, warning = FALSE}
train.named <- ddply(train, .(Name), summarise,
                     proportion = as.numeric(prop.table(table(OutcomeType))),
                     OutcomeType = names(table(OutcomeType)))
print(train.named)
ggplot(train.named, aes(OutcomeType, proportion, fill = Name)) +
        geom_bar(stat = "identity", position = "dodge") +
        scale_y_continuous(labels = percent) +
        labs(title = "Percentage in function of Named vs. Not Named Animals for each outcome", y = "Percentage", x = "Outcome Types")
```

Named animals are mostly adopted since no named animals are transfered from the above histogram. The feature telling if an animal has a name or not influances clearly the outcomes.


## 2.4 Impact of Age on Outcomes
We state that most important outcome will be different from a younger to an older animal. We define 'younger' animals all animals that are 1 month or younger, and 'older' animals being 1 year old and older.

```{r echo = TRUE, message = FALSE, warning = FALSE}
train.copy$AgeuponOutcome[train$AgeuponOutcome <= 30] <- "1 month and younger"
train.copy$AgeuponOutcome[train$AgeuponOutcome > 30 & train$AgeuponOutcome < 365] <- "Between 1 month and 1 year"
train.copy$AgeuponOutcome[train$AgeuponOutcome >= 365] <- "1 year and older"

train.age <- ddply(train.copy, .(AgeuponOutcome), summarise,
                   proportion = as.numeric(prop.table(table(OutcomeType))),
                   OutcomeType = names(table(OutcomeType)))
print(train.age)
ggplot(train.age, aes(OutcomeType, proportion, fill = AgeuponOutcome)) +
        geom_bar(stat = "identity", position = "dodge") +
        scale_y_continuous(labels = percent) +
        labs(title = "Percentage in function of the Age for each outcome", y = "Percentage", x = "Outcome Types")
```

We can see that older animals are mostly returned to the owner. The younger animals are transferred and the ones between 1 month and 1 year old are adopted. In conclusion, the age of animals has a significant influance on the outcomes. Therefore, creating groups of age like this will help.


## 2.5 Animal's Sterility
We want to see if extracting information that check if the animal is sterile or not will have influance on the outcomes. Generally, people who want to adopt a dog or a cat want to know if the animal is sterile or intact. Let's see if this is true with our dataset.

```{r echo = TRUE, message = FALSE, warning = FALSE}
train.copy$SexuponOutcome[grep("Intact", train$SexuponOutcome)] <- "Intact"
train.copy$SexuponOutcome[c(grep("Spayed", train$SexuponOutcome), grep("Neutered", train$SexuponOutcome))] <- "Sterile"
train.copy$SexuponOutcome[train$SexuponOutcome == 0] <- "Unknown"

train.sterile <- ddply(train.copy, .(SexuponOutcome), summarise,
                   proportion = as.numeric(prop.table(table(OutcomeType))),
                   OutcomeType = names(table(OutcomeType)))
print(train.sterile)
ggplot(train.sterile, aes(OutcomeType, proportion, fill = SexuponOutcome)) +
        geom_bar(stat = "identity", position = "dodge") +
        scale_y_continuous(labels = percent) +
        labs(title = "Percentage in function of Animals Sterility for each outcome", y = "Percentage", x = "Outcome Types")
```

We can see that sterile animals are mostly adopted which confirms our hypothesis. Unknown or intact animals are mostly transfered. Note that animals that we do not know if they are sterile or not (Unknown) are not adopted. Thus, knowing if the animal is sterile or not has influance on the outcomes.


## 2.6 Date & Time
The date and time may have a big influance on the outcomes. We have seen that doptions represent the most popular outcome of the dataset. Lets formulate some hypotheses on adoptions, transfers and returns to the owner. We state that

1. people will mostly adopt during the day Lets say that the day starts at 7am and ends at 22pm.
2. an animal is returned to the owner when an employee is present at the animal center. Thus, mostly at open hours of the center which is the day.
3. an animal is transferred out of open hours (during the night could be a good time) of the animal center since people can come visiting and maybe adopting an animal.
4. an animal may die at anytime of the day. 
5. an animal will be euthanasied mostly during the day. 

```{r echo = TRUE, message = FALSE, warning = FALSE}
train.copy$WeekDay <- weekdays(as.Date(train$DateTime))

train.weekday <- ddply(train.copy, .(WeekDay), summarise,
                       proportion = as.numeric(prop.table(table(OutcomeType))),
                       OutcomeType = names(table(OutcomeType)))
print(train.weekday)
ggplot(train.weekday, aes(OutcomeType, proportion, fill = WeekDay, width = 0.5)) +
        geom_bar(stat = "identity", position = "dodge") +
        scale_y_continuous(labels = percent) +
        labs(title = "Percentage in function of Weekdays for each outcomes", y = "Percentage", x = "Outcome Types")
```

From the bar chart, we can see that animals are mostly adopted the weekend (Saturday and Sunday). This makes sense since most of people are working from Monday to Friday. Also, the weekend, there is less euthanasias, transfers and returns to owner. Looking at the website [Austin Animal Center](http://www.austintexas.gov/department/animal-services), animal receiving are only from 11am - 5pm the weekend and 11am - 7pm the other days. From these information, we suppose that extracting the hour from the `DateTime` feature could have influance on the outcomes. At least, differentiate between before and after noon will be a good start.

```{r echo = TRUE, message = FALSE, warning = FALSE}
train.hours <- hour(ymd_hms(train$DateTime))
train.copy$IsAfternoon[train.hours >= 12] <- "PM"
train.copy$IsAfternoon[train.hours < 12] <- "AM"

train.isafternoon <- ddply(train.copy, .(IsAfternoon), summarise,
                           proportion = as.numeric(prop.table(table(OutcomeType))),
                           OutcomeType = names(table(OutcomeType)))
print(train.isafternoon)
ggplot(train.isafternoon, aes(OutcomeType, proportion, fill = IsAfternoon)) +
        geom_bar(stat = "identity", position = "dodge") +
        scale_y_continuous(labels = percent) +
        labs(title = "Percentage in function of before vs after noon for each outcome", y = "Percentage", x = "Outcome Types")
```

From the bar chart, transfers of animals are mostly done before noon which prove that hypothesis 3 is true. Adoptions are mostly done during the afternoon which prove that hypotheses 1, 2 and 5 are true. There is no significative difference for animals died, thus the hypothesis 4 is also true. 


```{r echo = TRUE, message = FALSE, warning = FALSE}
train.copy$Year <- year(ymd_hms(train$DateTime))

train.year <- ddply(train.copy, .(Year), summarise,
                      proportion = as.numeric(prop.table(table(OutcomeType))),
                      OutcomeType = names(table(OutcomeType)))
print(train.year)
ggplot(train.year, aes(OutcomeType, proportion, fill = Year)) +
    geom_bar(stat = "identity", position = "dodge") +
    scale_y_continuous(labels = percent) +
    labs(title = "Percentage in function of years for each outcome", y = "Percentage", x = "Outcome Types")
```


## 2.7 Animal Type
The objective is to replace every unique feature's values by their corresponding integer. Features like `AnimalType` and `OutcomeType` are replaced by their corresponding `ID` as specified in the Code Book.

```{r echo = TRUE, message = FALSE, warning = FALSE}
train.animal.type <- ddply(train, .(AnimalType), summarise,
                           proportion = as.numeric(prop.table(table(OutcomeType))),
                           OutcomeType = names(table(OutcomeType)))
print(train.animal.type)
ggplot(train.animal.type, aes(OutcomeType, proportion, fill = AnimalType)) +
        geom_bar(stat = "identity", position = "dodge") +
        scale_y_continuous(labels = percent) +
        labs(title = "Percentage in function of Animal Types for each outcome", y = "Percentage", x = "Outcome Types")

train$AnimalType <- CategoryToInteger(train$AnimalType)
test$AnimalType <- CategoryToInteger(test$AnimalType)
```


# 3 Feature Engineering
In this section, we add and remove features from the dataset. We also transform each feature in categories represented by a positive integer. The objective is to clean and prepare the dataset for our prediction's model and to answer the questions: `Are there features that can be split in many other features? If yes, are they improving the score? Why and how?`. The answer to those questions will be the conclusion of this section.


## 3.1 Missing / Unknown Values
All blank, space and unknown values are replaced by the value `0`. TO DO: Explain WHY??


## 3.2 Removing ID and OutcomeSubtype
We remove the features `AnimalID`, `OutcomeSubtype` from the test and train sets.

```{r echo = TRUE, message = FALSE, warning = FALSE}
train$AnimalID <- NULL
test.id <- test$ID
test$ID <- NULL

train$OutcomeSubtype <- NULL
```


## 3.3 Adding Features
The objective is to look at each feature of the dataset and split them if there are more information given than what the feature's name tells. For example, the feature `SexuponOutcome` tells us more than just the sex of the animal regarding the possible values.


### 3.3.1 DateTime Feature
From the feature `DateTime` of the dataset, we extract the date and the time separately. We replace the `DateTime` feature by 2 new features named `Date` and `Time`. Those features are transformed to positive integers. From them, we can also extract other information like:

* Day (integer from 0 to 29 depending on the month)
* Month (integer from 0 to 11)
* Year (integer from 2013 to 2016 respectively represented by 0 to 3)
* Hour (integer from 0 to 23)
* Weekday (integer from 0 to 6 which represent Sunday to Saturday)
* Number of days from the oldest date of the dataset
* Before or afternoon (between midnight to noon or between noon to midnight)
* The time represented by the equation $60h + m$ where $h$ is the hours and $m$ the minutes.

```{r echo = TRUE, message = FALSE, warning = FALSE}
train$WeekDay <- as.POSIXlt(train$DateTime)$wday
test$WeekDay <- as.POSIXlt(test$DateTime)$wday

train.date <- ymd_hms(train$DateTime)
test.date <- ymd_hms(test$DateTime)
years <- unique(c(year(train.date), year(test.date)))

## The year starts at 0 which is the minimum year of the dataset.
train$Year <- year(train.date) - min(years)
train$Month <- month(train.date)
train$Day <- day(train.date)
train$DateInDays <- difftime(as.Date(train.date,'%Y/%m/%d'), as.Date(min(train.date),'%Y/%m/%d'), units = c("days"))
train$Hours <- hour(train.date)
train$Time <- train$Hours * 60 + minute(train.date)
train$IsPm <- ifelse(train$Hours >= 12, 1, 0)
#train.birthday <- train.date - days(train$AgeuponOutcome)
#train$Birthday <- difftime(as.Date(train.birthday,'%Y/%m/%d'), as.Date(min(train.birthday),'%Y/%m/%d'), units = c("days"))

test$Year <- year(test.date) - min(years)
test$Month <- month(test.date)
test$Day <- day(test.date)
test$DateInDays <- difftime(as.Date(test.date,'%Y/%m/%d'), as.Date(min(test.date),'%Y/%m/%d'), units = c("days"))
test$Hours <- hour(test.date)
test$Time <- test$Hours * 60 + minute(test.date)
test$IsPm <- ifelse(test$Hours >= 12, 1, 0)
#test.birthday <- test.date - days(test$AgeuponOutcome)
#test$Birthday <- difftime(as.Date(test.birthday,'%Y/%m/%d'), as.Date(min(test.birthday),'%Y/%m/%d'), units = c("days"))

train$DateTime <- NULL
test$DateTime <- NULL
```


### 3.3.2 AgeuponOutcome Feature
We have seen that age of animals separated in 3 categories has influance on outcomes.

```{r echo = TRUE, message = FALSE, warning = FALSE}
train$AgeGroup <- train$AgeuponOutcome
train$AgeGroup[train$AgeuponOutcome <= 30] <- 0
train$AgeGroup[train$AgeuponOutcome <= 365 & train$AgeuponOutcome > 30] <- 1
train$AgeGroup[train$AgeuponOutcome > 365] <- 2

test$AgeGroup <- test$AgeuponOutcome
test$AgeGroup[test$AgeuponOutcome <= 30] <- 0
test$AgeGroup[test$AgeuponOutcome <= 365 & test$AgeuponOutcome > 30] <- 1
test$AgeGroup[test$AgeuponOutcome > 365] <- 2


# train$AgeInYears <- train$AgeuponOutcome / 365
# train$AgeInYears[train$AgeuponOutcome < 1] <- 0
# 
# test$AgeInYears <- test$AgeuponOutcome / 365
# test$AgeInYears[test$AgeuponOutcome < 1] <- 0
```


### 3.3.3 SexuponOutcome Feature
This feature contains information on the sex of the animal and if the animal is sterile or intact. Thus, we create two features named `Sex` and `Sterility` that will replace the feature `SexuponOutcome`.

```{r echo = TRUE, message = FALSE, warning = FALSE}
train.sex.male.list <- grep("Male", train$SexuponOutcome)
train.sex.female.list <- grep("Female", train$SexuponOutcome)
train$Sex <- 0
train$Sex[train.sex.male.list] <- 1
train$Sex[train.sex.female.list] <- 2

test.sex.male.list <- grep("Male", test$SexuponOutcome)
test.sex.female.list <- grep("Female", test$SexuponOutcome)
test$Sex <- 0
test$Sex[test.sex.male.list] <- 1
test$Sex[test.sex.female.list] <- 2


train.sex.intacts <- grep("Intact", train$SexuponOutcome)
train.sex.steriles <- c(grep("Spayed", train$SexuponOutcome), grep("Neutered", train$SexuponOutcome))
train$Sterility <- rep(0, length(train$SexuponOutcome))
train$Sterility[train.sex.intacts] <- 1
train$Sterility[train.sex.steriles] <- 2

test.sex.intacts <- grep("Intact", test$SexuponOutcome)
test.sex.steriles <- c(grep("Spayed", test$SexuponOutcome), grep("Neutered", test$SexuponOutcome))
test$Sterility <- rep(0, length(test$SexuponOutcome))
test$Sterility[test.sex.intacts] <- 1
test$Sterility[test.sex.steriles] <- 2

train$SexuponOutcome <- NULL
test$SexuponOutcome <- NULL
```


### 3.3.4 Breed Feature
For each animal, we verify if it is a mixed breed, a purebred or a crossed breed. We extract this information from the `Breed` feature and add a new feature `BreedType` where all purebred animals are identified with the value 2 per the code book. The mixed breed are identified with the value 0 and crossed breed with value 1.

```{r echo = TRUE, message = FALSE, warning = FALSE}
train.breed.mix.list <- grep(" Mix", train$Breed)
train.breed.cross.list <- grep("/", train$Breed)
train$BreedType <- 2
train$BreedType[train.breed.cross.list] <- 1
train$BreedType[train.breed.mix.list] <- 0

test.breed.mix.list <- grep(" Mix", test$Breed)
test.breed.cross.list <- grep("/", test$Breed)
test$BreedType <- 2
test$BreedType[test.breed.cross.list] <- 1
test$BreedType[test.breed.mix.list] <- 0
```

We want to get all possible breeds from the dataset. To process that, we will remove the string `Mix` for mixed breed animals and we will separate breeds from the crossed breed animals. Those are identified by a slash character `/`.

```{r echo = TRUE, message = FALSE, warning = FALSE}
# train.breed1 <- gsub(" Mix|/.*", "", train$Breed)
# train.breeds <- unique(c(gsub(" Mix|.*/", "", train$Breed), train.breed1))
# indices <- grep("/", train$Breed)
# 
# train$Breed1 <- GetIntegerFeatureFromGroups(train.breed1, train.breeds)
# train.breed2 <- rep("", length(train$Breed))
# train.breed2[indices] <- gsub(" Mix|.*/", "", train$Breed[indices])
# train$Breed2 <- GetIntegerFeatureFromGroups(train.breed2, train.breeds)
# 
# 
# test.breed1 <- gsub(" Mix|/.*", "", test$Breed)
# test.breeds <- unique(c(gsub(" Mix|.*/", "", test$Breed), test.breed1))
# indices <- grep("/", test$Breed)
# 
# test$Breed1 <- GetIntegerFeatureFromGroups(test.breed1, test.breeds)
# test.breed2 <- rep("", length(test$Breed))
# test.breed2[indices] <- gsub(" Mix|.*/", "", test$Breed[indices])
# test$Breed2 <- GetIntegerFeatureFromGroups(test.breed2, test.breeds)

## Get Cat Breed
#cat.breed <- unique(train$Breed[train$AnimalType == "Cat"])

## New feature 'HairType' where Shorthair = 1, Longhair = 2, Wirehair = 3, Medium Hair = 4, otherwise = 0
hair.type <- c("Shorthair", "Longhair", "Wirehair", "Medium Hair")
train$HairType <- GetIntegerFeatureFromGroups(train$Breed, hair.type)
test$HairType <- GetIntegerFeatureFromGroups(test$Breed, hair.type)

## New feature 'IsDomestic' where Domestic = 1 otherwise = 0
train$IsDomestic <- GetBooleanFeatureFromGroup(train$Breed, "Domestic")
test$IsDomestic <- GetBooleanFeatureFromGroup(test$Breed, "Domestic")

## New feature 'IsMiniature' where Miniature = 1 otherwise = 0
train$IsMiniature <- GetBooleanFeatureFromGroup(train$Breed, "Miniature")
test$IsMiniature <- GetBooleanFeatureFromGroup(test$Breed, "Miniature")

## New feature 'IsPitbull' where Pit Bull = 1 otherwise = 0
train$IsPitbull <- GetBooleanFeatureFromGroup(train$Breed, "Pit Bull")
test$IsPitbull <- GetBooleanFeatureFromGroup(test$Breed, "Pit Bull")

## New feature 'IsHound' where Hound = 1 otherwise = 0
#train$IsHound <- GetBooleanFeatureFromGroup(train$Breed, "Hound")
#test$IsHound <- GetBooleanFeatureFromGroup(test$Breed, "Hound")

## New feature 'IsLabrador' where Labrador Retriever = 1 otherwise = 0
#train$IsLabrador <- GetBooleanFeatureFromGroup(train$Breed, "Labrador")
#test$IsLabrador <- GetBooleanFeatureFromGroup(test$Breed, "Labrador")

## New feature 'IsChihuahua' where Chihuahua = 1 otherwise = 0
#train$IsChihuahua <- GetBooleanFeatureFromGroup(train$Breed, "Chihuahua")
#test$IsChihuahua <- GetBooleanFeatureFromGroup(test$Breed, "Chihuahua")

## New feature 'IsTerrier' where Terrier = 1 otherwise = 0
#train$IsTerrier <- GetBooleanFeatureFromGroup(train$Breed, "Terrier")
#test$IsTerrier <- GetBooleanFeatureFromGroup(test$Breed, "Terrier")

#train$IsShepherd <- GetBooleanFeatureFromGroup(train$Breed, "Shepherd")
#test$IsShepherd <- GetBooleanFeatureFromGroup(test$Breed, "Shepherd")

train$Breed <- NULL
test$Breed <- NULL
```


### 3.3.5 Colors
With the color feature, we can extract the number of colors. If we find a slash `/` character which we define as a `separator`, then we have 2 colors. If we find `Tricolor`, then this means that we have 3 colors. 

```{r echo = TRUE, message = FALSE, warning = FALSE}
train.tricolor <- grep("Tricolor", train$Color)
train$NumberOfColors <- str_count(train$Color, "/") + 1
train$NumberOfColors[train.tricolor] <- 3

test.tricolor <- grep("Tricolor", test$Color)
test$NumberOfColors <- str_count(test$Color, "/") + 1
test$NumberOfColors[test.tricolor] <- 3
```

As we have seen previously, we know that if there is a separator, then we have 2 colors. Thus, we can split the feature `Color` in 2 features: `Color1` and `Color2`. To get `Color1`, we will take all strings without the separator or the left side of the string if a separator is found. For `Color2`, we will take the right side of all strings containing the separator. Those features will be converted as a positive integer where each represents a specific color. `Color2` is 0 if and only if there is no separator.

```{r echo = TRUE, message = FALSE, warning = FALSE}
train.colors <- unique(c(gsub(".*/", "", train$Color), gsub("/.*", "", train$Color)))
train.colors.list <- unique(gsub(" .*$", "", train.colors))
indices <- grep("/", train$Color)

train.color1 <- gsub("/.*", "", train$Color)
train$Color1 <- GetIntegerFeatureFromGroups(train.color1, train.colors.list)

train.color2 <- rep("", length(train$Color))
train.color2[indices] <- gsub(".*/", "", train$Color[indices])
train$Color2 <- GetIntegerFeatureFromGroups(train.color2, train.colors.list)


test.colors <- unique(c(gsub(".*/", "", test$Color), gsub("/.*", "", test$Color)))
test.colors.list <- unique(gsub(" .*$", "", test.colors))
indices <- grep("/", test$Color)

test.color1 <- gsub("/.*", "", test$Color)
test$Color1 <- GetIntegerFeatureFromGroups(test.color1, test.colors.list)

test.color2 <- rep("", length(test$Color))
test.color2[indices] <- gsub(".*/", "", test$Color[indices])
test$Color2 <- GetIntegerFeatureFromGroups(test.color2, test.colors.list)

train$Color <- NULL
test$Color <- NULL
```

We can extract qualifier of colors for every color containing a space character which we define as the qualifier separator. Since we have 2 features for the colors, we will have 2 features for qualifier as well. We name these features `ColorQualifier1` and `ColorQualifier2` which are associated to `Color1` and `Color2` respectively. These 2 features are set to 0 if there is no qualifier.

```{r echo = TRUE, message = FALSE, warning = FALSE}
colors.indices <- grep(" ", train.colors)
train.qualifiers <- unique(gsub(".* ", "", train.colors[colors.indices]))

indices <- grep(" ", train.color1)
train$ColorQualifier1 <- rep("", length(train.color1))
train$ColorQualifier1[indices] <- gsub(".* ", "", train.color1[indices])
train$ColorQualifier1 <- GetIntegerFeatureFromGroups(train$ColorQualifier1, train.qualifiers)

# indices <- grep(" ", train.color2)
# train$ColorQualifier2 <- rep("", length(train.color2))
# train$ColorQualifier2[indices] <- gsub(".* ", "", train.color2[indices])
# train$ColorQualifier2 <- GetIntegerFeatureFromGroups(train$ColorQualifier2, train.qualifiers)


colors.indices <- grep(" ", test.colors)
test.qualifiers <- unique(gsub(".* ", "", test.colors[colors.indices]))

indices <- grep(" ", test.color1)
test$ColorQualifier1 <- rep("", length(test.color1))
test$ColorQualifier1[indices] <- gsub(".* ", "", test.color1[indices])
test$ColorQualifier1 <- GetIntegerFeatureFromGroups(test$ColorQualifier1, test.qualifiers)

# indices <- grep(" ", test.color2)
# test$ColorQualifier2 <- rep("", length(test.color2))
# test$ColorQualifier2[indices] <- gsub(".* ", "", test.color2[indices])
# test$ColorQualifier2 <- GetIntegerFeatureFromGroups(test$ColorQualifier2, test.qualifiers)
```

Since the second qualifier is not usefull, we removed it from the train and test sets.


# 4 Training Model
In this section, we present what algorithm is used, why are we using it and how it is used. The objective is to build the final submission file containing the probabilities for each outcome type of each animal given in the test set. 


## 4.1 Fine Tuning Parameters
We prepare the parameters and matrices for the cross-validation and final prediction. Since we need to get the probabilities for each class, then the objective used is `multi:softprob`. We have found that the evaluation metric used is the multi-class log loss function which is `mlogloss` for the `XGBoost` algorithm.

```{r echo = TRUE, message = FALSE, warning = FALSE}
outcomes.string <- sort(unique(train$OutcomeType))
indices <- order(unique(train$OutcomeType))
outcome.type <- CategoryToInteger(train$OutcomeType) - 1
train$OutcomeType <- NULL

outcome.class.num <- length(outcomes.string)
param <- list(objective        = "multi:softprob",
              num_class        = outcome.class.num,    # number of classes
              eta              = 0.085,      # Control the learning rate
              subsample        = 0.75,     # Subsample ratio of the training instance
              max_depth        = 8,        # Maximum depth of the tree
              colsample_bytree = 0.85,     # Subsample ratio of columns when constructing each tree
              eval_metric      = "mlogloss")
```


## 4.2 Cross-Validation
We do a cross-validation to get the optimal number of trees and multi-class log-loss score.

```{r echo = FALSE, message = FALSE, warning = FALSE}
## Cross-Validation
cv.nfolds <- 10
cv.nrounds <- 250

train.matrix <- xgb.DMatrix(data.matrix(train), label = outcome.type)
model.cv <- xgb.cv(data     = train.matrix, 
                   nfold    = cv.nfolds, 
                   param    = param,
                   nrounds  = cv.nrounds, 
                   verbose  = 0)
model.cv$names <- as.integer(rownames(model.cv))

cv.plot.title <- paste("Training log-loss using", cv.nfolds, "folds CV")
print(ggplot(model.cv, aes(x = names, y = test.mlogloss.mean)) + 
      geom_line() + 
      ggtitle(cv.plot.title) + 
      xlab("Number of trees") + 
      ylab("log-loss"))
     
print(model.cv)
best <- model.cv[model.cv$test.mlogloss.mean == min(model.cv$test.mlogloss.mean), ]
cat("\nOptimal testing set Log-Loss score:", best$test.mlogloss.mean)
cat("\nApprox. Kaggle Log-Loss score:", best$test.mlogloss.mean - (best$test.mlogloss.std / 2))
cat("\nInterval testing set Log-Loss score: [", best$test.mlogloss.mean - best$test.mlogloss.std, ", ", best$test.mlogloss.mean + best$test.mlogloss.std, "].")
cat("\nDifference between optimal training and testing sets Log-Loss:", best$train.mlogloss.mean - best$test.mlogloss.mean)
cat("\nOptimal number of trees:", best$names)
```


## 4.3 Prediction
We proceed to the predictions of the test set. After testing, this number of trees seems to be optimal with the parameters given above.

```{r echo = FALSE, message = FALSE, warning = FALSE}
nrounds <- as.integer(best$names)

model = xgboost(param = param, 
                train.matrix, 
                nrounds = nrounds,
                verbose = 0,
                probability = TRUE)

test.matrix <- xgb.DMatrix(data.matrix(test))

prediction.test <- predict(model, test.matrix)
prediction.train <- predict(model, train.matrix)

#Check which features are the most important.
names <- dimnames(train)[[2]]
importance.matrix <- xgb.importance(names, model = model)
print(importance.matrix)

# Display the features importance.
print(xgb.plot.importance(importance.matrix))
```


## 4.4 Multi-class Log-Loss
We can verify how our predictions score under the multi-class log-loss function. We take our predictions applied to the train set and we compare to the real `OutcomeType` values of the train set.

```{r echo = FALSE, message = FALSE, warning = FALSE}
prediction.train <- matrix(prediction.train, ncol = outcome.class.num, byrow = TRUE)

rows <- nrow(train)
outcomes <- matrix(0, nrow = rows, ncol = outcome.class.num)
for(i in 1:rows)
{
    outcomes[i ,outcome.type[i] + 1] <- 1
}

multiclass.logloss <- MultiLogLoss(prediction.train, outcomes)
print(multiclass.logloss)
```


## 4.5 Submission
We write the `ID` and the predicted outcome classes in the submission file.

```{r echo = TRUE, message = FALSE, warning = FALSE}
prediction.test <- data.frame(matrix(prediction.test, ncol = outcome.class.num, byrow = TRUE))[, indices]
colnames(prediction.test) <- outcomes.string
#prediction.test <- CreateDataframeFromThreshold(prediction.test, 0.5)
submission <- cbind(data.frame(ID = test.id), prediction.test)
write.csv(submission, "Submission.csv", row.names = FALSE)
```